\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{geometry}
\usepackage{fontspec}
\setmainfont{Times New Roman}

\geometry{top=2cm, bottom=2cm, left=2cm, right=2cm}

\newtheorem{theorem}{Теорема}
\newtheorem{lemma}{Лемма}
\newtheorem{definition}{Определение}
\newtheorem{remark}{Замечание}

\begin{document}

\section*{Расширенный конспект по теории оптимизации}

\subsection*{1. Общая формулировка задачи оптимизации и аналитическая сложность минимизации невыпуклых липшицевых функций}
\textbf{Формулировка задачи оптимизации:}
\[
\min_{x \in \mathbb{R}^n} f(x),
\]
где \(f(x)\) --- целевая функция, заданная на \(\mathbb{R}^n\), и может быть выпуклой/невыпуклой, гладкой/негладкой.

\textbf{Нижняя граница сложности для невыпуклых \(L\)-липшицевых функций:}
\begin{theorem}
Для функций \(f(x)\), имеющих \(L\)-липшицев градиент (\(\|\nabla f(x) - \nabla f(y)\| \leq L\|x - y\|\)), методам первого порядка требуется \(\Omega(\epsilon^{-2})\) итераций для нахождения \(\epsilon\)-оптимального решения (\(\|\nabla f(x)\| \leq \epsilon\)).
\end{theorem}

\textbf{Доказательство:} Строится контрпример на основе квадратичной функции \(f(x) = \frac{L}{2}x^2\), где вычисление градиентов показывает невозможность достижения точности за меньшее число итераций. Полное доказательство опускается в виду сложности.

\subsection*{2. Выпуклая оптимизация и примеры в машинном обучении}
\textbf{Определение выпуклой функции:}
Функция \(f(x)\) выпукла, если:
\[
f(\lambda x + (1 - \lambda)y) \leq \lambda f(x) + (1-\lambda)f(y), \quad \forall x, y \text{ и } \lambda \in [0, 1].
\]

\textbf{Примеры выпуклых задач:}
\begin{itemize}
    \item \textbf{Регрессия (Lasso):}
    \[
    \min_w \|Xw - y\|_2^2 + \lambda \|w\|_1.
    \]
    Здесь \(\|w\|_1\) является выпуклой, но негладкой нормой.
    \item \textbf{Логистическая регрессия:}
    \[
    \min_w \frac{1}{N} \sum_{i=1}^N \log(1 + e^{-y_i w^\top x_i}) + \lambda \|w\|_2^2.
    \]
    \item \textbf{Поддерживающие векторы (SVM):}
    \[
    \min_w \frac{1}{2}\|w\|_2^2 + C \sum_{i=1}^N \max(0, 1 - y_i w^\top x_i).
    \]
\end{itemize}

\subsection*{3. Адаптивный градиентный спуск и наискорейший спуск}
\textbf{Адаптивный градиентный спуск:}
Методы семейства AdaGrad, RMSProp и Adam используют информацию о предыдущих градиентах для масштабирования шагов:
\[
t_t \propto \frac{1}{\sqrt{\sum_{i=1}^t g_i^2}}.
\]

\textbf{Наискорейший спуск:}
\begin{definition}
На каждом шаге выбирается \(t_k\), минимизирующий \(f(x_k - t \nabla f(x_k))\) по \(t\).
\end{definition}

\begin{theorem}
При \(L\)-липшицевом градиенте наискорейший спуск обладает линейной скоростью сходимости для строго выпуклых функций.
\end{theorem}

\subsection*{4. Градиентный метод при условии градиентного доминирования (Поляка-Лоясиевича)}
\textbf{Определение условия Поляка-Лоясиевича:}
\[
\|\nabla f(x)\|^2 \geq 2\mu (f(x) - f^*), \quad \mu > 0.
\]

\begin{theorem}
Градиентный метод для функций, удовлетворяющих условию Поляка-Лоясиевича, сходится со скоростью:
\[
f(x_k) - f^* \leq \frac{C}{k^2}.
\]
\end{theorem}

\textbf{Пример:} Обучение глубокой нейронной сети, где перепараметризация вводит скрытую регуляризацию, улучшая поведение градиентов.

\subsection*{5. Стохастический градиентный метод (SGD)}
\textbf{Идея:} Вместо вычисления полного градиента используется приближенный градиент на случайной выборке:
\[
\nabla f_i(x).
\]

\begin{theorem}
При уменьшении шага по правилу \(t_k = O(1/\sqrt{k})\), SGD достигает сходимости:
\[
\mathbb{E}[f(x_k)] - f^* = O(1/\sqrt{k}).
\]
\end{theorem}

\textbf{Применение:} Эффективен для обучения больших моделей.

\subsection*{6. Неточный оракул и минибатчинг}
\textbf{Неточный оракул:} Приближает градиенты с заданной точностью:
\[
\|\nabla f(x) - g(x)\| \leq \epsilon.
\]

\textbf{Минибатчинг:} Выбирается подмножество данных, что снижает шум, но сохраняет стохастичность.

\subsection*{7. Ускоренные градиентные методы}
\textbf{Метод Нестерова:}
\[
y_{k+1} = x_k + \frac{k-1}{k+2}(x_k - x_{k-1}).
\]

\begin{theorem}
Скорость сходимости для выпуклых функций:
\[
f(x_k) - f^* \leq O(1/k^2).
\]
\end{theorem}

\subsection*{8. Метод сопряжённых градиентов}
\textbf{Для задач:}
\[
\min_x \frac{1}{2}x^\top Qx - b^\top x.
\]
Используется структура матрицы \(Q\) для ускорения сходимости.

\subsection*{9. Метод Франк-Вульфа}
\begin{theorem}
Скорость сходимости метода:
\[
f(x_k) - f^* \leq O(1/k).
\]
\end{theorem}

\subsection*{10. Субградиентный метод}
\begin{theorem}
Для выпуклых задач:
\[
f(x_k) - f^* \leq O(1/\sqrt{k}).
\]
\end{theorem}

\subsection*{11. Универсальные методы}
Методы, работающие как для гладких, так и для негладких задач. Пример: ускоренные универсальные схемы Нестерова.

\subsection*{12. Стохастический субградиентный метод}
Работает аналогично SGD, но применим для негладких задач.

\subsection*{13. AdaGrad}
\[
t_t \propto \frac{1}{\sqrt{\sum_{i=1}^t g_i^2}}.
\]

\subsection*{14. Метод Ньютона}
Использует гессиан для квадратичной сходимости.

\subsection*{15. Квазиньютоновские методы}
Метод BFGS: аппроксимирует гессиан с помощью информации о градиентах.

\end{document}

