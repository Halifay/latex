\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[T2A]{fontenc}
\usepackage{fontspec}
\setmainfont{Times New Roman}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{geometry}
\geometry{top=2cm, bottom=2cm, left=2cm, right=2cm}


\begin{document}

\section*{Теория оптимизации: подробный конспект}

\subsection*{1. Общая формулировка задачи оптимизации и аналитическая сложность минимизации невыпуклых липшицевых функций}
\textbf{Формулировка задачи оптимизации:}
\[
\min_{x \in \mathbb{R}^n} f(x),
\]
где \(f(x)\) --- целевая функция, заданная на \(\mathbb{R}^n\), и может быть выпуклой/невыпуклой, гладкой/негладкой.

\textbf{Нижняя граница сложности:} Для невыпуклых \(L\)-липшицевых функций (\(f\) с \(L\)-липшицевым градиентом) методам требуется \(\Omega(\epsilon^{-2})\) итераций для нахождения \(\epsilon\)-оптимального решения (\(\|\nabla f(x)\| \leq \epsilon\)).

\subsection*{2. Выпуклая оптимизация и примеры в машинном обучении}
\textbf{Определение:} Функция \(f(x)\) выпукла, если
\[
f(\lambda x + (1 - \lambda)y) \leq \lambda f(x) + (1-\lambda)f(y), \quad \forall x, y \text{ и } \lambda \in [0, 1].
\]

\textbf{Примеры в машинном обучении:}
\begin{itemize}
    \item \textbf{Регрессия (Lasso, Ridge):}
    \[
    \min_w \|Xw - y\|_2^2 + \lambda \|w\|_1.
    \]
    \item \textbf{Логистическая регрессия:}
    \[
    \min_w \frac{1}{N} \sum_{i=1}^N \log(1 + e^{-y_i w^\top x_i}) + \lambda \|w\|_2^2.
    \]
    \item \textbf{SVM с линейным ядром:}
    \[
    \min_w \frac{1}{2}\|w\|_2^2 + C \sum_{i=1}^N \max(0, 1 - y_i w^\top x_i).
    \]
\end{itemize}

\subsection*{3. Адаптивный градиентный спуск и наискорейший спуск}
\textbf{Адаптивный градиентный спуск:}
\begin{itemize}
    \item Приспосабливает шаг \(t\) к характеристикам задачи, учитывая историю градиентов.
    \item Пример: Adam, AdaGrad.
\end{itemize}

\textbf{Наискорейший спуск:}
\begin{itemize}
    \item На каждой итерации минимизируется \(f(x - t \nabla f(x))\) по \(t\).
    \item Пример: Применяется в выпуклой оптимизации, где возможно аналитическое нахождение оптимального \(t\).
\end{itemize}

\subsection*{4. Градиентный метод для задач с градиентным доминированием (Поляка-Лоясиевича)}
\textbf{Условие Поляка-Лоясиевича:}
\[
\|\nabla f(x)\|^2 \geq 2\mu (f(x) - f^*), \quad \mu > 0.
\]

\textbf{Пример:} Обучение глубокой нейронной сети через нелинейные перепараметризованные слои; ускоренная сходимость благодаря свойству градиентного доминирования.

\subsection*{5. Стохастический градиентный метод (SGD)}
\textbf{Суть:} Замена градиента \(\nabla f(x)\) его приближением на случайной выборке:
\[
\nabla f_i(x).
\]
\textbf{Применение:} Эффективен для задач с большими размерами данных (например, обучение нейронных сетей).

\subsection*{6. Неточный оракул и минибатчинг}
\textbf{Неточный оракул:} Предоставляет приближенные градиенты, обеспечивая:
\[
\|\nabla f(x) - g(x)\| \leq \epsilon.
\]

\textbf{Минибатчинг:} Используется для уменьшения шума стохастических градиентов; вычисляется на небольшой выборке данных.

\subsection*{7. Ускоренные градиентные методы и метод подобных треугольников}
\textbf{Метод Нестерова:}
\begin{itemize}
    \item \textbf{Ускорение:} \(O(1/k^2)\) для выпуклых задач.
    \item \textbf{Схема:} 
    \[
    y_{k+1} = x_k + \frac{k-1}{k+2}(x_k - x_{k-1}).
    \]
\end{itemize}

\textbf{Метод подобных треугольников:} Использует геометрическое представление, применим к гладким выпуклым задачам.

\subsection*{8. Метод сопряжённых градиентов}
\textbf{Цель:} Решение задач квадратичной оптимизации:
\[
\min_x \frac{1}{2}x^\top Qx - b^\top x.
\]
\textbf{Сходимость:} Зависит от числа обусловленности \(\kappa(Q)\).

\subsection*{9. Метод Франк-Вульфа}
\textbf{Суть:} Решение задач:
\[
\min f(x) \text{ при ограничении } x \in \mathcal{D},
\]
где \(\mathcal{D}\) --- выпуклое множество.

\textbf{Сходимость:} \(O(1/k)\) для гладких функций.

\subsection*{10. Субградиентный метод}
\textbf{Оценка сходимости:}
\[
O(1/\sqrt{k})
\]
для выпуклых задач.

\subsection*{11. Универсальные методы градиентного типа}
\textbf{Суть:} Применимы к широкому классу задач (гладкие/негладкие). Пример: ускоренный универсальный метод.

\subsection*{12. Стохастический субградиентный метод}
\textbf{Особенность:} Применим к негладким задачам. Использует выборку для оценки субградиента.

\subsection*{13. AdaGrad}
\textbf{Идея:} Регулировка шагов \(t\) на основе накопленной информации о градиентах:
\[
t \propto \frac{1}{\sqrt{\sum g_t^2}}.
\]
\textbf{Пример:} Обучение моделей с разреженными градиентами.

\subsection*{14. Метод Ньютона}
\textbf{Схема:} Использует гессиан \(H\):
\[
x_{k+1} = x_k - H^{-1}\nabla f(x_k).
\]
\textbf{Сходимость:} Квадратичная для выпуклых функций.

\subsection*{15. Квазиньютоновские методы}
\textbf{Суть:} Аппроксимация гессиана \(H\) без явного вычисления. 
\textbf{Пример:} Метод BFGS, L-BFGS.

\end{document}

